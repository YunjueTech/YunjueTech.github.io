---
title: 'Yunjue Agent：面向开放域的原位自进化智能体'
date: '2026-01-26'
lastmod: '2026-01-26'
tags: []
draft: false
summary: 'SOTA级别自进化'
authors:
  - default
---


在 AI Agent 的研究中，面对开放环境（Open-ended Environments），传统系统往往面临着“任务分布持续漂移”与“外部监督稀缺”的双重挑战。依赖静态工具库或离线训练的 Agent 难以适应动态变化的真实世界需求。

为此，我们提出了 **Yunjue Agent**。这是一篇关于 **In-Situ Self-Evolution（原位自演化）** 范式的技术报告。该系统能够在 Zero-Start（零起点）条件下，将离散的推理交互重构为连续的经验流，实现能力的实时“蒸馏”与复用。

本文将深入解析该论文的核心亮点、演化机制以及对未来 Agent 研究的展望。

## 核心亮点 (Highlights)

### 1. In-situ Self-evolving：推理即演化的新范式

Yunjue Agent 打破了传统“离线训练-在线部署”的静态边界。我们提出了一种**原位自演化（In-Situ Self-Evolving）**框架，针对 Open-ended 环境中任务分布持续漂移的问题，构建了一套无需 Ground Truth 的内部反馈循环机制。

* **实时性：** 这种进化不是以“epoch”为单位，而是以单次 Query 为单位。系统在处理完第 $t$ 个问题后，立即利用这次的经验修改系统配置（$\mathcal{M}_{t}$），直接用于解决第 $t+1$ 个问题。
* **无监督演化：** 系统不依赖外部标注数据。它将短期的推理执行反馈实时“蒸馏”为长期的可复用能力，实现对未知领域边界的持续探索与自适应迭代。

### 2. Zero-Start 下的 SOTA 性能

为了验证系统的极限能力，我们设定了严苛的 **Tabula Rasa（白板/零起点）** 实验环境：**系统初始化为空工具库**。

这意味着 Agent 必须完全依赖推理期间的生成、验证与归纳来构建能力。实验结果显示：
* **显著增益：** 在五个基准测试中，Yunjue Agent 实现了相对于 Backend 模型（如 Gemini 3 Pro）的显著绝对能力增益。例如，在 DeepSearchQA 上提升了 **+17.4%**。
* **HLE 世界第二：** 在高难度的 Humanity's Last Exam (HLE) 基准上，我们的表现仅次于 OpenAI 的 GPT-5.2 Pro。
* **通用原语涌现：** 分析显示，系统自发涌现出了高频调用的通用原语（如 `search_web`, `evaluate_math_expression`），证明了从零自举（Bootstrapping）构建跨域通用能力层的可行性。

### 3. 工具优先 (Tool-First) 的演化路径

在 Agent 的三大支柱——工作流 (Workflow)、记忆 (Memory)、工具 (Tool)——中，我们论证了**工具演化**在无监督场景下具有不可替代的第一性原理地位。

* **客观的二元反馈 (Binary Feedback)：** 相比于 Memory 可能带来的幻觉或 Workflow 评估的主观性，工具执行提供了最宝贵的判别信号——代码要么运行成功，要么报错（Traceback）。这构成了客观的内部监督信号。
* **能力边界定义：** 工具直接定义了 Agent “能做什么”。
* **避免策略偏差：** 通过优先演化工具，我们避免了在早期将不可靠的经验固化为错误的记忆或流程。只有当工具库收敛后，才应当考虑基于此进行更高层级的摘要和 Workflow 优化。

### 4. Fully Reproducible：白箱化的研究基座

为了将“黑箱式 Agent 成果”转化为可追溯、可对比的公共研究资产，我们坚持**完全开源与可复现**：

* **全量 Artifacts：** 我们开源了完整代码、所有 Benchmark 的运行脚本与评测对齐方式。
* **演化痕迹 (Traces)：** 我们提供了每一步工具生成、修改、合并的版本化产物，以及全量的交互日志。
* **审计与研究：** 研究者可以精确审计每次能力增长来自哪些工具、哪些失败被修复、哪些工具被淘汰，并据此开展如工具质量判别、收敛分析、演化效率等更细粒度的研究。

---

## 深度解析：原位自演化 vs. 普通自演化

为了更清晰地界定 Yunjue Agent 的技术定位，我们将 In-situ Self-evolving 与目前主流的 Self-evolving Agent 进行了对比：

| 维度 | 普通 Self-evolving Agent | In-situ Self-evolving Agent (Yunjue) |
| :--- | :--- | :--- |
| **发生阶段 (Phase)** | 通常发生在**训练阶段 (Training Process)**，或者是离线的优化过程。 | 发生在**推理阶段 (Inference Phase)**，即系统实际运行并处理用户请求的当下。 |
| **监督信号 (Signals)** | 依赖**外部监督信号**或**真值 (Ground Truth)**。通过对比输出与标准答案来最大化目标函数（如准确率）。 | **无外部监督，无真值**。依赖“内部反馈”（如工具执行结果）或从上一次交互中获得的经验进行自我调整。 |
| **更新机制 (Mechanism)** | 针对特定任务或数据集进行一轮轮的迭代优化（Iterative/Batch）。 | **动态且连续 (Dynamic & Continuous)**。是一种“边做边学”的模式，经验即刻生效。 |

---

## 未来工作 (Future Work)

虽然本研究验证了通过工具合成实现原位自演化的有效性，但这仅仅是通向更高级自主智能体的第一步。我们在论文中指出了几个关键的未来研究方向：

### 1. 范式转移：迈向 Agent 系统的系统级预训练
工具库清晰的收敛曲线表明，“解题能力”不仅是特设启发式方法的集合，更是一种可学习、可蒸馏的通用模式。
这预示着 Agent 系统即将迎来类似 LLM 的**“预训练 (Pre-training) + 后训练 (Post-training)”**时代。我们设想未来的多智能体系统将在海量、宽谱的任务数据集上进行系统级预训练，在部署前蒸馏出一套收敛的**“基础工具集 (Foundation Toolset)”**。这种预训练 Agent 将具备内在的泛化能力，通过组合现有可靠工具来解决下游新任务，从而最小化测试时演化的成本。

### 2. 记忆与工作流的协同演化 (Co-evolution)
目前的框架主要通过工具生成来验证自演化。然而，对于需要高度个性化（如个人助理）或复杂流程管理（如深度研究）的场景，仅有工具是不够的。
未来的关键方向是将演化机制扩展到**记忆架构**和**工作流策略**的协同演化，使系统能够随着功能能力的提升，同步调整其内部状态管理和执行逻辑。

### 3. 演化稳定性与正则化
由于 LLM 生成的随机性，工具集在不同实验运行中可能存在差异。确保工具库的一致收敛对系统可靠性至关重要。未来的工作将专注于开发正则化策略，以保证开放环境演化过程的确定性。

### 4. 并行 Batch 演化的优化
我们的 Batch Evolution 策略仍有优化空间：
* **课程学习效应：** 研究最佳的 Query 排序，避免因过早处理困难任务而阻碍基础原语的形成。
* **Batch 内多样性权衡：** 平衡“低多样性带来的 Best-of-N 质量保证”与“高多样性带来的演化效率”。
* **自适应调度：** 开发能根据收敛信号动态调整 Batch Size 的自主 Agent。

---

*获取论文全文、代码及详细 Traces，请访问[Github代码](https://github.com/YunjueTech/Yunjue-Agent/tree/main) 。*